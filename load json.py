from airflow import DAG
from airflow.operators.python import PythonOperator
import json
import requests
import logging
import time
import os
from airflow.operators.trigger_dagrun import TriggerDagRunOperator
from airflow.utils.dates import days_ago
from datetime import timedelta

def save_unique_players_to_json():
    api_key = '3f7d70c4-f8ca-42c9-98cb-3d1bdcc34ba7'
    headers = {'Authorization': f'Bearer {api_key}'}
    all_players_path = '/opt/airflow/dags/unique_players.json'
    new_players_path = '/opt/airflow/dags/new_players.json'

    if os.path.exists(all_players_path):
        with open(all_players_path, 'r') as f:
            existing_ids = set(json.load(f))
    else:
        existing_ids = set()

    with open('/opt/airflow/dags/new_champ.json', 'r') as f:
        championships_data = json.load(f)

    new_ids = set()
    total_matches = 0

    for i, championship_id in enumerate(championships_data):
        logging.info(f"[{i+1}/{len(championships_data)}] ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ñ‡ÐµÐ¼Ð¿Ð¸Ð¾Ð½Ð°Ñ‚Ð° {championship_id}")
        offset = 0
        limit = 100
        type = 'past'

        while True:
            url = f'https://open.faceit.com/data/v4/championships/{championship_id}/matches?type={type}&offset={offset}&limit={limit}'
            response = requests.get(url, headers=headers)
            time.sleep(0.1)

            if response.status_code != 200:
                logging.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð·Ð°Ð¿Ñ€Ð¾ÑÐ° Ð¼Ð°Ñ‚Ñ‡ÐµÐ¹ Ð´Ð»Ñ Ñ‡ÐµÐ¼Ð¿Ð¸Ð¾Ð½Ð°Ñ‚Ð° {championship_id}: {response.status_code}")
                break

            matches = response.json().get('items', [])
            if not matches:
                break

            total_matches += len(matches)

            for match in matches:
                for team in match.get('teams', {}).values():
                    for player in team.get('roster', []):
                        player_id = player.get('player_id')
                        if player_id and player_id not in existing_ids:
                            new_ids.add(player_id)

            logging.info(f"  -> {len(matches)} Ð¼Ð°Ñ‚Ñ‡ÐµÐ¹ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð½Ð¾, Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾ Ð½Ð¾Ð²Ñ‹Ñ… Ð¸Ð³Ñ€Ð¾ÐºÐ¾Ð²: {len(new_ids)}")

            if len(matches) < limit:
                break
            offset += limit

    combined_ids = list(existing_ids.union(new_ids))

    with open(all_players_path, 'w') as f:
        json.dump(combined_ids, f, indent=4)

    with open(new_players_path, 'w') as f:
        json.dump(list(new_ids), f, indent=4)

    logging.info(f"ðŸ†• Ð”Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ð¾ {len(new_ids)} Ð½Ð¾Ð²Ñ‹Ñ… player_id. Ð’ÑÐµÐ³Ð¾ Ð² Ñ„Ð°Ð¹Ð»Ðµ: {len(combined_ids)}. ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð½Ð¾ Ð¼Ð°Ñ‚Ñ‡ÐµÐ¹: {total_matches}")


# === DAG Definition ===
default_args = {
    'owner': 'airflow',
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    'save_unique_players_to_json',
    description='Fetch unique players and save them to JSON (with new_players.json)',
    schedule_interval=None,
    start_date=days_ago(1),
    catchup=False,
    default_args=default_args,
)

save_players_task = PythonOperator(
    task_id='save_unique_players_to_json',
    python_callable=save_unique_players_to_json,
    dag=dag,
    execution_timeout=timedelta(minutes=30),
)

# Ð¢Ñ€Ð¸Ð³Ð³ÐµÑ€ Ð²Ñ‚Ð¾Ñ€Ð¾Ð³Ð¾ DAG
trigger_task = TriggerDagRunOperator(
    task_id='trigger_second_dag',
    trigger_dag_id='load_players_to_postgres',
    dag=dag,
    wait_for_completion=True,
)

save_players_task >> trigger_task
